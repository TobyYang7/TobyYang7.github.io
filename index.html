<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yuzhe YANG </title> <meta name="author" content="Yuzhe YANG"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/cuhk.png?8150a4c40057f359da05258868ade7fc"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tobyyang7.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%74%6F%62%79%79%61%6E%67%37@%66%6F%78%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope fa-xs"></i></a> <a href="https://scholar.google.com/citations?user=Oj296F8AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/TobyYang7" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github fa-xs"></i></a> <a href="https://www.linkedin.com/in/tobyyang7" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yuzhe</span> YANG </h1> <p class="desc"><a href="https://sds.cuhk.edu.cn/en" rel="external nofollow noopener" target="_blank">Senior Student, School of Data Science<br>The Chinese University of Hong Kong, Shenzhen</a> </p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_new-480.webp 480w,/assets/img/prof_new-800.webp 800w,/assets/img/prof_new-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_new.jpg?7432e91814e8ff67feafc3b0a60938ac" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_new.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <div style="font-size: small; text-align: center;">Beijing, 2019 Winter</div> <div style="font-size: medium; text-align: center; font-family: Cursive;">We imagine our own importance.<br>We invent our purpose.<br>We are nothing.</div> </div> </div> <div class="clearfix"> <p><img src="https://komarev.com/ghpvc/?username=TobyYang7" alt=""> <a href="https://hits.seeyoufarm.com" rel="external nofollow noopener" target="_blank"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Ftobyyang7.github.io&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=hits&amp;edge_flat=false" alt="Hits"></a></p> <p>Hi, this is Toby’s homepage. I am a senior student at <a href="https://www.cuhk.edu.cn/en" rel="external nofollow noopener" target="_blank">The Chinese University of Hong Kong, Shenzhen</a>, majoring in Computer Science and Engineering. During my undergraduate studies, I am fortunately advised by <a href="https://wabyking.github.io/old.html" rel="external nofollow noopener" target="_blank">Prof. Benyou Wang</a> and <a href="https://sds.cuhk.edu.cn/en/teacher/268" rel="external nofollow noopener" target="_blank">Prof. Jianfeng Mao</a>.</p> <p>Previously, my research was focused on : <strong>(1) Graph Learning, (2) Multimodal LLM, (3) Trustworthy AI.</strong> Additionally, I am experimenting with integrating LLM with graph to explore new possibilities.</p> <p><strong>I am seeking a PhD or RA position for Fall 2025.</strong> If you are interested in my research or have any questions, please feel free to contact me.</p> <div style="text-align: center;"> <a href="mailto:yuzheyang@link.cuhk.edu.cn">Email</a> / <a href="https://tobyyang7.github.io/assets/pdf/yuzhe_cv.pdf" rel="external nofollow noopener" target="_blank">Resume</a> / <a href="https://github.com/TobyYang7" rel="external nofollow noopener" target="_blank">GitHub</a> / <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=Oj296F8AAAAJ&amp;view_op=list_works&amp;gmla=AETOMgF_Tp1r9sOUDnRnv5h82tY7x2KspWTMX4T0-sWjkMRssLRn9LzEupX4v8OVLHfRjaW6V5dm-FNc9hgZN9A7otxAUD-SdQBPxoNUiXJdgavBzaQ" rel="external nofollow noopener" target="_blank">Google Scholar</a> </div> <p><br> <br></p> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/FAST-CA-480.webp 480w,/assets/img/publication_preview/FAST-CA-800.webp 800w,/assets/img/publication_preview/FAST-CA-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/FAST-CA.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FAST-CA.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="LI2024102326" class="col-sm-8"> <div class="title">FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction</div> <div class="author"> Chi Li ,  Xixian Qi ,  <em>Yuzhe Yang</em> ,  Zhuo Zeng ,  Lianmin Zhang ,  and  Jianfeng Mao </div> <div class="periodical"> <em>Information Fusion</em>, 2024 </div> <div class="periodical"> SOTA spatio-temporal model for predicting airport network delay propagation </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1566253524001040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/FAST-CA.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The issue of delay propagation prediction in airport networks has garnered increasing global attention, particularly due to its profound impact on operational efficiency and passenger satisfaction in modern air transportation systems. Despite research advancements in this domain, existing methodologies often fall short of comprehensively addressing the challenges associated with predicting delay propagation in airport networks, especially in terms of handling complex spatial-temporal dependencies and sequence couplings. In response to the complex challenge of predicting delay propagation in airport networks, we introduce the Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention (FAST-CA) framework. FAST-CA is an innovative model that integrates dynamic and adaptive graph learning, coupled attention mechanisms, periodicity feature extraction, and multifaceted information fusion modules. This holistic approach enables a thorough analysis of the interplay between flight departure and arrival delays and the spatial-temporal correlations within airport networks. Rigorously evaluated on two extensive real-world datasets, our model consistently outperforms current state-of-the-art baseline models, showcasing superior predictive performance and the effective learning capabilities of its intricately designed modules. Our research highlights the criticality of analyzing spatial-temporal relationships and the dynamics of flight coupling, offering significant theoretical and practical contributions to the advancement and management of air transportation systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">LI2024102326</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Information Fusion}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102326}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1566-2535}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.inffus.2024.102326}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1566253524001040}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Chi and Qi, Xixian and Yang, Yuzhe and Zeng, Zhuo and Zhang, Lianmin and Mao, Jianfeng}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Graph neural networks, Flight delay prediction, Delay propagation, Dynamic graph, Adaptive learning}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{SOTA spatio-temporal model for predicting airport network delay propagation}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/finllava-480.webp 480w,/assets/img/publication_preview/finllava-800.webp 800w,/assets/img/publication_preview/finllava-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/finllava.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="finllava.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="xie2024openfinllmsopenmultimodallarge" class="col-sm-8"> <div class="title">Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</div> <div class="author"> Qianqian Xie ,  Dong Li ,  Mengxi Xiao ,  Zihao Jiang ,  Ruoyu Xiang ,  Xiao Zhang ,  Zhengyu Chen ,  Yueru He ,  Weiguang Han ,  <em>Yuzhe Yang</em> , and <span class="more-authors" title="click to view 29 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '29 more authors' ? 'Shunian Chen, Yifei Zhang, Lihang Shen, Daniel Kim, Zhiwei Liu, Zheheng Luo, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Yilun Zhao, Yitao Long, Guojun Xiong, Kaleb Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jianyun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Jimin Huang, Sophia Ananiadou' : '29 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '20'); ">29 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> First open-source financial multimodal LLM: FinLLaVA-8B </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/papers/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.thefin.ai/home" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have advanced financial applications, yet they often lack sufficient financial knowledge and struggle with tasks involving multi-modal inputs like tables and time series data. To address these limitations, we introduce Open-FinLLMs, a series of Financial LLMs. We begin with FinLLaMA, pre-trained on a 52 billion token financial corpus, incorporating text, tables, and time-series data to embed comprehensive financial knowledge. FinLLaMA is then instruction fine-tuned with 573K financial instructions, resulting in FinLLaMA-instruct, which enhances task performance. Finally, we present FinLLaVA, a multimodal LLM trained with 1.43M image-text instructions to handle complex financial data types. Extensive evaluations demonstrate FinLLaMA’s superior performance over LLaMA3-8B, LLaMA3.1-8B, and BloombergGPT in both zero-shot and few-shot settings across 19 and 4 datasets, respectively. FinLLaMA-instruct outperforms GPT-4 and other Financial LLMs on 15 datasets. FinLLaVA excels in understanding tables and charts across 4 multimodal tasks. Additionally, FinLLaMA achieves impressive Sharpe Ratios in trading simulations, highlighting its robust financial application capabilities. We will continually maintain and improve our models and benchmarks to support ongoing innovation in academia and industry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xie2024openfinllmsopenmultimodallarge</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Qianqian and Li, Dong and Xiao, Mengxi and Jiang, Zihao and Xiang, Ruoyu and Zhang, Xiao and Chen, Zhengyu and He, Yueru and Han, Weiguang and Yang, Yuzhe and Chen, Shunian and Zhang, Yifei and Shen, Lihang and Kim, Daniel and Liu, Zhiwei and Luo, Zheheng and Yu, Yangyang and Cao, Yupeng and Deng, Zhiyang and Yao, Zhiyuan and Li, Haohang and Feng, Duanyu and Dai, Yongfu and Somasundaram, VijayaSai and Lu, Peng and Zhao, Yilun and Long, Yitao and Xiong, Guojun and Smith, Kaleb and Yu, Honghai and Lai, Yanzhao and Peng, Min and Nie, Jianyun and Suchow, Jordan W. and Liu, Xiao-Yang and Wang, Benyou and Lopez-Lira, Alejandro and Huang, Jimin and Ananiadou, Sophia}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{First open-source financial multimodal LLM: FinLLaVA-8B}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 02, 2024</th> <td> My first publication is accepted! You can find the paper <a href="https://doi.org/10.1016/j.inffus.2024.102326" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 27, 2024</th> <td> Created this website using <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. <strong>To be continued…</strong> </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">Latest Posts</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 03, 2024</th> <td> <a class="news-title" href="/blog/2024/Sunset-Serenade-Over-Victoria-Harbour/">Sunset Serenade Over Victoria Harbour</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 02, 2024</th> <td> <a class="news-title" href="/blog/2024/MTGNN-structure/">MTGNN structure</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 28, 2024</th> <td> <a class="news-title" href="/blog/2024/HDR-Photo-Gallery/">HDR Photo Gallery</a> </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yuzhe YANG. Last updated: September 05, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YE837RNHM5"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YE837RNHM5");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
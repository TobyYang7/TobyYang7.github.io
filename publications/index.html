<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Yuzhe YANG </title> <meta name="author" content="Yuzhe YANG"> <meta name="description" content="&lt;p&gt;(* Equal contribution)&lt;/p&gt;"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/cuhk.png?8150a4c40057f359da05258868ade7fc"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tobyyang7.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuzhe </span> YANG </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Projects</h1> <p class="post-description"></p> <p>(* Equal contribution)</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/UCFE-480.webp 480w,/assets/img/publication_preview/UCFE-800.webp 800w,/assets/img/publication_preview/UCFE-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/UCFE.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="UCFE.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024ucfeusercentricfinancialexpertise" class="col-sm-8"> <div class="title">UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</div> <div class="author"> <em>Yuzhe Yang*</em> ,  Yifei Zhang* ,  Yan Hu ,  Yilin Guo ,  Ruoli Gan ,  Yueru He ,  Mingcong Lei ,  Xiao Zhang ,  Haining Wang ,  Qianqian Xie , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jimin Huang, Honghai Yu, Benyou Wang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '20'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> A User-Centric framework designed to evaluate LLMs’ ability to handle complex financial tasks </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.14059" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/papers/2410.14059" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2410.14059" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/TobyYang7/UCFE-Benchmark" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, taskspecific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2024ucfeusercentricfinancialexpertise</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang*, Yuzhe and Zhang*, Yifei and Hu, Yan and Guo, Yilin and Gan, Ruoli and He, Yueru and Lei, Mingcong and Zhang, Xiao and Wang, Haining and Xie, Qianqian and Huang, Jimin and Yu, Honghai and Wang, Benyou}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{q-fin.CP}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2410.14059}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{A User-Centric framework designed to evaluate LLMs' ability to handle complex financial tasks}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/FAST-CA-480.webp 480w,/assets/img/publication_preview/FAST-CA-800.webp 800w,/assets/img/publication_preview/FAST-CA-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/FAST-CA.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FAST-CA.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="LI2024102326" class="col-sm-8"> <div class="title">FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction</div> <div class="author"> Chi Li ,  Xixian Qi ,  <em>Yuzhe Yang</em> ,  Zhuo Zeng ,  Lianmin Zhang ,  and  Jianfeng Mao </div> <div class="periodical"> <em>Information Fusion</em>, 2024 </div> <div class="periodical"> SOTA spatio-temporal model for predicting airport network delay propagation </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1566253524001040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/FAST-CA.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The issue of delay propagation prediction in airport networks has garnered increasing global attention, particularly due to its profound impact on operational efficiency and passenger satisfaction in modern air transportation systems. Despite research advancements in this domain, existing methodologies often fall short of comprehensively addressing the challenges associated with predicting delay propagation in airport networks, especially in terms of handling complex spatial-temporal dependencies and sequence couplings. In response to the complex challenge of predicting delay propagation in airport networks, we introduce the Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention (FAST-CA) framework. FAST-CA is an innovative model that integrates dynamic and adaptive graph learning, coupled attention mechanisms, periodicity feature extraction, and multifaceted information fusion modules. This holistic approach enables a thorough analysis of the interplay between flight departure and arrival delays and the spatial-temporal correlations within airport networks. Rigorously evaluated on two extensive real-world datasets, our model consistently outperforms current state-of-the-art baseline models, showcasing superior predictive performance and the effective learning capabilities of its intricately designed modules. Our research highlights the criticality of analyzing spatial-temporal relationships and the dynamics of flight coupling, offering significant theoretical and practical contributions to the advancement and management of air transportation systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">LI2024102326</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FAST-CA: Fusion-based Adaptive Spatial-Temporal Learning with Coupled Attention for airport network delay propagation prediction}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Information Fusion}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102326}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1566-2535}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.inffus.2024.102326}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1566253524001040}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Chi and Qi, Xixian and Yang, Yuzhe and Zeng, Zhuo and Zhang, Lianmin and Mao, Jianfeng}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Graph neural networks, Flight delay prediction, Delay propagation, Dynamic graph, Adaptive learning}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{SOTA spatio-temporal model for predicting airport network delay propagation}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/finllava-480.webp 480w,/assets/img/publication_preview/finllava-800.webp 800w,/assets/img/publication_preview/finllava-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/finllava.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="finllava.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xie2024openfinllmsopenmultimodallarge" class="col-sm-8"> <div class="title">Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</div> <div class="author"> Qianqian Xie ,  Dong Li ,  Mengxi Xiao ,  Zihao Jiang ,  Ruoyu Xiang ,  Xiao Zhang ,  Zhengyu Chen ,  Yueru He ,  Weiguang Han ,  <em>Yuzhe Yang</em> , and <span class="more-authors" title="click to view 29 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '29 more authors' ? 'Shunian Chen, Yifei Zhang, Lihang Shen, Daniel Kim, Zhiwei Liu, Zheheng Luo, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Yilun Zhao, Yitao Long, Guojun Xiong, Kaleb Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jianyun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Jimin Huang, Sophia Ananiadou' : '29 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '20'); ">29 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> First open-source financial multimodal LLM: FinLLaVA-8B </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/papers/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2408.11878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.thefin.ai/home" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have advanced financial applications, yet they often lack sufficient financial knowledge and struggle with tasks involving multi-modal inputs like tables and time series data. To address these limitations, we introduce Open-FinLLMs, a series of Financial LLMs. We begin with FinLLaMA, pre-trained on a 52 billion token financial corpus, incorporating text, tables, and time-series data to embed comprehensive financial knowledge. FinLLaMA is then instruction fine-tuned with 573K financial instructions, resulting in FinLLaMA-instruct, which enhances task performance. Finally, we present FinLLaVA, a multimodal LLM trained with 1.43M image-text instructions to handle complex financial data types. Extensive evaluations demonstrate FinLLaMA’s superior performance over LLaMA3-8B, LLaMA3.1-8B, and BloombergGPT in both zero-shot and few-shot settings across 19 and 4 datasets, respectively. FinLLaMA-instruct outperforms GPT-4 and other Financial LLMs on 15 datasets. FinLLaVA excels in understanding tables and charts across 4 multimodal tasks. Additionally, FinLLaMA achieves impressive Sharpe Ratios in trading simulations, highlighting its robust financial application capabilities. We will continually maintain and improve our models and benchmarks to support ongoing innovation in academia and industry.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xie2024openfinllmsopenmultimodallarge</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Qianqian and Li, Dong and Xiao, Mengxi and Jiang, Zihao and Xiang, Ruoyu and Zhang, Xiao and Chen, Zhengyu and He, Yueru and Han, Weiguang and Yang, Yuzhe and Chen, Shunian and Zhang, Yifei and Shen, Lihang and Kim, Daniel and Liu, Zhiwei and Luo, Zheheng and Yu, Yangyang and Cao, Yupeng and Deng, Zhiyang and Yao, Zhiyuan and Li, Haohang and Feng, Duanyu and Dai, Yongfu and Somasundaram, VijayaSai and Lu, Peng and Zhao, Yilun and Long, Yitao and Xiong, Guojun and Smith, Kaleb and Yu, Honghai and Lai, Yanzhao and Peng, Min and Nie, Jianyun and Suchow, Jordan W. and Liu, Xiao-Yang and Wang, Benyou and Lopez-Lira, Alejandro and Huang, Jimin and Ananiadou, Sophia}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{First open-source financial multimodal LLM: FinLLaVA-8B}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/feddtpt-480.webp 480w,/assets/img/publication_preview/feddtpt-800.webp 800w,/assets/img/publication_preview/feddtpt-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/feddtpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="feddtpt.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wu2024feddtptfederateddiscretetransferable" class="col-sm-8"> <div class="title">FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models</div> <div class="author"> Jiaqi Wu ,  Simin Chen ,  <em>Yuzhe Yang</em> ,  Yijiang Li ,  Shiyue Hou ,  Rui Jing ,  Zehua Wang ,  Wei Chen ,  and  Zijian Tian </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> A federated prompt tuning method for black-box LLMs, enhancing privacy, efficiency, and performance on non-iid data </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.00985" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2411.00985" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, large language models (LLMs) have significantly advanced the field of natural language processing (NLP). By fine-tuning LLMs with data from specific scenarios, these foundation models can better adapt to various downstream tasks. However, the fine-tuning process poses privacy leakage risks, particularly in centralized data processing scenarios. To address user privacy concerns, federated learning (FL) has been introduced to mitigate the risks associated with centralized data collection from multiple sources. Nevertheless, the privacy of LLMs themselves is equally critical, as potential malicious attacks challenge their security, an issue that has received limited attention in current research. Consequently, establishing a trusted multi-party model fine-tuning environment is essential. Additionally, the local deployment of large LLMs incurs significant storage costs and high computational demands. To address these challenges, we propose for the first time a federated discrete and transferable prompt tuning, namely FedDTPT, for black-box large language models. In the client optimization phase, we adopt a token-level discrete prompt optimization method that leverages a feedback loop based on prediction accuracy to drive gradient-free prompt optimization through the MLM API. For server optimization, we employ an attention mechanism based on semantic similarity to filter all local prompt tokens, along with an embedding distance elbow detection and DBSCAN clustering strategy to enhance the filtering process. Experimental results demonstrate that, compared to state-of-the-art methods, our approach achieves higher accuracy, reduced communication overhead, and robustness to non-iid data in a black-box setting. Moreover, the optimized prompts are transferable.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2024feddtptfederateddiscretetransferable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Jiaqi and Chen, Simin and Yang, Yuzhe and Li, Yijiang and Hou, Shiyue and Jing, Rui and Wang, Zehua and Chen, Wei and Tian, Zijian}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2411.00985}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{A federated prompt tuning method for black-box LLMs, enhancing privacy, efficiency, and performance on non-iid data}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/FDPT-480.webp 480w,/assets/img/publication_preview/FDPT-800.webp 800w,/assets/img/publication_preview/FDPT-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/FDPT.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FDPT.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="FDPT" class="col-sm-8"> <div class="title">FDPT: Federated Discrete Prompt Tuning for Black-Box Visual-Language Models</div> <div class="author"> Jiaqi Wu ,  Simin Chen ,  Jing Tang ,  <em>Yuzhe Yang</em> ,  Lixu Wang ,  Song Lin ,  Zehua Wang ,  Wei Chen ,  and  Zijian Tian </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> A federated discrete prompt tuning method for black-box VLMs, improving accuracy and efficiency while protecting privacy </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://tobyyang7.github.io/assets/pdf/FDPT.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>General-purpose Vision-Language Models (VLMs) have driven major advancements in multimodal AI. Fine-tuning these models with task-specific data enhances adaptability to various downstream tasks but suffers from privacy risks. While potential solutions like federated learning can address user data privacy concerns, model protection is also essential. Other methods that rely on black-box VLM APIs usually require the access of prediction logits, leaving them open to inversion attacks. Moreover, addressing the challenges of tuning complexity and data transmission efficiency in federated VLM scenarios is also crucial. To address these challenges, we propose FDPT—a federated discrete prompt tuning method utilizing black-box VLM APIs. The black-box VLM is restricted to output text only, without relying on traditional prediction logits. In the client optimization stage, we design an AI agent-driven, token-level prompt optimization method that employs a Generate-Feedback mechanism to iteratively learn task experience. Additionally, we use an Exploration-Exploitation Balance strategy to drive a Progressive Prompt Refinement Chain-of-Thought, adaptively controlling the optimization scale. In the global aggregation stage, we propose a Semantic-similarity-guided Evolutionary Computation method, filtering local discrete tokens based on semantic similarity to enable unsupervised selection of representative tokens. Experimental results show that, compared to eight state-of-the-art methods, FDPT significantly improves accuracy in traditional image classification and visual question-answer tasks, reduces communication overhead, and produces highly transferable optimized prompts. Moreover, it demonstrates greater robustness to data heterogeneity.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Quant-GPT-480.webp 480w,/assets/img/publication_preview/Quant-GPT-800.webp 800w,/assets/img/publication_preview/Quant-GPT-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/Quant-GPT.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Quant-GPT.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024quantgpt" class="col-sm-8"> <div class="title">Quant-GPT: Money is All You Need</div> <div class="author"> <em>Yuzhe Yang</em> ,  Kangqi Yu ,  and  Juanquan Peng </div> <div class="periodical"> 2024 </div> <div class="periodical"> A Large Language Model for A-share Market Investment </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Quant_GPT_final_report.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TobyYang7/Quant-GPT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/Quant-GPT_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This paper introduces Quant-GPT, a novel multi-agent optimized for Ashare market investment decisions. Leveraging a fine-tuning combination of distilled sentiment analysis from ChatGPT and real-world market data, the prediction agent of Quant-GPT addresses the challenges of model collapse and weak causality between sentiment and expected returns. Our methodology integrates a Retrieval-Augmented Generation (RAG) agent and summary agent, enabling the model to access relevant news articles and corporate announcements summary with concise investment information to enhance investment decision-making. The inclusion of diverse datasets and RAG significantly improves the model’s ability to forecast market trends and returns accurately. Experimental results demonstrate Quant-GPT’s superior performance over existing open-source LLMs in terms of annualized return, maximum draw-down, and Sharpe ratio. These findings underscore the potential of advanced language models in financial applications, providing a robust framework for integrating natural language understanding with quantitative investment strategies. The code is available on GitHub: https://github.com/TobyYang7/Quant-GPT</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Llava_Qwen2-480.webp 480w,/assets/img/publication_preview/Llava_Qwen2-800.webp 800w,/assets/img/publication_preview/Llava_Qwen2-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/Llava_Qwen2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Llava_Qwen2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="llava_qwen2" class="col-sm-8"> <div class="title">LLaVA-Qwen2: Enhanced with Qwen2 Base Model</div> <div class="author"> <em>Yuzhe Yang</em> </div> <div class="periodical"> 2024 </div> <div class="periodical"> An extension of LLaVA integrating the Qwen2 model for enhanced visual instruction tuning. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/TobyYang7/Llava_Qwen2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>LLaVA-Qwen2 enhances the LLaVA project by incorporating the Qwen2 base model, aiming to bring GPT-4 level capabilities to visual instruction tuning. This repository provides a custom implementation that leverages the advanced features of Qwen2 for improved performance in large language and vision models. The project includes datasets for pretraining and finetuning, evaluation scripts, and a user-friendly interface. The result is a robust system designed for sophisticated multimodal tasks in various domains, particularly focused on financial applications with the FinVis dataset. Detailed instructions for installation, usage, and model evaluation are provided.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iba-480.webp 480w,/assets/img/publication_preview/iba-800.webp 800w,/assets/img/publication_preview/iba-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/iba.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iba.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024insurancegpt" class="col-sm-8"> <div class="title">Travel Insurance Recommendation AI System Based on Flight Delay Predictions and Customer Sentiment</div> <div class="author"> <em>Yuzhe Yang</em> ,  Haoqi Zhang ,  Zhidong Peng ,  Yilin Guo ,  and  Tianji Zhou </div> <div class="periodical"> 2024 </div> <div class="periodical"> Predicting Purchase Intentions for Dynamic Insurance Pricing </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Travel_Delay_Insurance_Recommendation_AI_System.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TobyYang7/Travel-Insurance-Recommendation-AI-System" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cuhko365-my.sharepoint.com/:p:/g/personal/121020064_link_cuhk_edu_cn/EeZSiXgHQB9Dq-2wFqcVypUBuUREppF42pGmyjBsHWRVqw?e=6h7Zxc&amp;nav=eyJzSWQiOjM1MCwiY0lkIjoyMDI3ODI4Nzk3fQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>In this project, we designed an AI system to identify potential travel insurance intentions of customers. Our designed large language model (LLM), named Insurance-GPT, is capable of analyzing in real-time during interactions with users and it utilizes deep learning model to accurately predict flight delay. This provides a good user experience, as well as provides a reference for pricing strategies to insurance companies. The Insurance-GPT can be downloaded at https://modelscope.cn/models/TobyYang7/ InsuranceGPT. Additionally, the complete source code for this project is available on GitHub at https://github.com/TobyYang7/ Travel-Insurance-Recommendation-AI-System.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/plane-480.webp 480w,/assets/img/publication_preview/plane-800.webp 800w,/assets/img/publication_preview/plane-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/plane.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="plane.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="flight_information_system" class="col-sm-8"> <div class="title">Flight Information System</div> <div class="author"> <em>Yuzhe Yang</em> ,  Zitong Wang ,  Baoyin Zhang ,  Haoqi Zhang ,  Jianzhen Chen ,  and  Zhidong Peng </div> <div class="periodical"> 2024 </div> <div class="periodical"> A project focused on enhancing aviation sector data management through a relational database system, integrated with LLM for optimized query generation and user-friendly web interface. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/tobyyang7/flight-information-system" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Flight Information System project aims to revolutionize data management in the aviation sector by developing a sophisticated relational database system. This system, grounded in an Entity-Relationship (E-R) model, is designed to streamline airline operations, including passenger bookings and flight logistics, ensuring data integrity and operational efficiency. By integrating Large Language Models (LLM), the project enhances the database architecture and query generation process, facilitating more intuitive interactions. The outcome is a functional, user-friendly web interface that replicates the complexities of airline management, offering both administrative and user-level interactions.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/map_pop-480.webp 480w,/assets/img/publication_preview/map_pop-800.webp 800w,/assets/img/publication_preview/map_pop-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/map_pop.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="map_pop.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="light_pollution_ahp_2023" class="col-sm-8"> <div class="title">Evaluation Model of Light Pollution by Multi-conditional AHP</div> <div class="author"> <em>Yuzhe Yang</em> ,  Jianzhen Chen ,  and  Zhongbin Chen </div> <div class="periodical"> 2023 </div> <div class="periodical"> Performed GIS data analysis and developed a mathematical model to evaluate light pollution, considering factors such as population data and regional income. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Evaluation%20Model%20of%20Light%20Pollution%20by%20Multi-conditional%20AHP.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Artificial light, while convenient, poses significant risks, including disruption of circadian rhythms in wildlife and severe health consequences for humans. California’s escalating light pollution problem exemplifies the need for effective solutions. To address this, we developed a light pollution risk assessment model focusing on economic, demographic, and ecological impacts, using indicators such as GDP per capita, population density, and the biological abundance index. The model, weighted by the analytic hierarchy process (AHP), was tested in California’s diverse regions—protected areas, rural, suburban, and urban settings. Our findings suggest that interventions, particularly limiting light sources with specific wavelengths, can significantly reduce light pollution risk, especially in suburban areas, demonstrating a risk reduction of up to 2.63%. These results highlight the importance of targeted strategies in mitigating light pollution effectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kaggle2023-480.webp 480w,/assets/img/publication_preview/kaggle2023-800.webp 800w,/assets/img/publication_preview/kaggle2023-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/kaggle2023.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kaggle2023.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="howard2022nfl" class="col-sm-8"> <div class="title">Kaggle Competition: 1st and Future - Player Contact Detection</div> <div class="author"> <em>Yuzhe Yang</em> </div> <div class="periodical"> 2023 </div> <div class="periodical"> Bronze Medal </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://kaggle.com/competitions/nfl-player-contact-detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Hosted by the National Football League (NFL), this competition aimed to detect player contacts using video and player tracking data to improve player safety.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/EIE3280-480.webp 480w,/assets/img/publication_preview/EIE3280-800.webp 800w,/assets/img/publication_preview/EIE3280-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/EIE3280.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="EIE3280.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2023seo" class="col-sm-8"> <div class="title">Game Theory Analysis of SEO Strategies: From Methods to Models</div> <div class="author"> <em>Yuzhe Yang</em> ,  Xiayu Ni ,  and  RongXin Cao </div> <div class="periodical"> 2023 </div> <div class="periodical"> This project examines SEO strategies through the lens of game theory, developing a model to enhance website visibility and ranking. The work involves both theoretical analysis and practical implementation, offering insights into effective SEO techniques. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/EIE3280.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/TobyYang7/EIE3280_Project" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This project delves into the application of game theory to Search Engine Optimization (SEO) strategies, focusing on enhancing a website’s importance score through various optimization techniques. The study includes the development and implementation of a customized optimization model, comparative analysis of different SEO methods, and validation through real-world system implementation. The findings demonstrate how specific SEO strategies can effectively improve a website’s visibility and ranking in search engine results.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/FTC2017-480.webp 480w,/assets/img/publication_preview/FTC2017-800.webp 800w,/assets/img/publication_preview/FTC2017-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/FTC2017.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FTC2017.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2017FTC" class="col-sm-8"> <div class="title">FIRST® Tech Challenge Game: RELIC RECOVERY</div> <div class="author"> RDFZ Robotic Team: Prototype </div> <div class="periodical"> 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.firstinspires.org/sites/default/files/uploads/resource_library/ftc/2017-2018/first-spectatorflyer18-ftc-ltr-dec-form.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/FTC2016-480.webp 480w,/assets/img/publication_preview/FTC2016-800.webp 800w,/assets/img/publication_preview/FTC2016-1400.webp 1400w," sizes="300" type="image/webp"></source> <img src="/assets/img/publication_preview/FTC2016.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="FTC2016.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2015FTC" class="col-sm-8"> <div class="title">FIRST® Tech Challenge Game: RES-Q</div> <div class="author"> JDFZ Robotic Team: UNICORN 1225 </div> <div class="periodical"> 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.firstinspires.org/sites/default/files/uploads/resource_library/ftc/first-res-q/first-resq-one-page.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuzhe YANG. Last updated: January 26, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YE837RNHM5"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YE837RNHM5");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </body> </html>